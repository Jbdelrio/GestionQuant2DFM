{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fred_transform(data: pd.DataFrame, transformations: dict[str, int]) -> pd.DataFrame:\n",
    "    \"\"\" \n",
    "    Transforme les séries de données macro-économiques selon les recommandations fournies par McCracken et Ng (2016).\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    data : pd.DataFrame\n",
    "        Tableau de données macro-économiques extraties de la base fred-md.\n",
    "     transformations : dict[str, int]\n",
    "        Dictionnaire ayant pour clé le nom de la série et pour valeur son code de transformation associé.\n",
    "    \"\"\"\n",
    "    # Création du DataFrame stockant les séries transformées\n",
    "    result = pd.DataFrame()\n",
    "    # Transformation des séries selon les codes fournis par McCracken et Ng (2016)\n",
    "    for col in data.columns:\n",
    "        # Récupération du code de transformation\n",
    "        code = transformations[col]\n",
    "        # Récupération de la série individuelle\n",
    "        subset = data[col]\n",
    "\n",
    "        # Application des transformées selon le code correspondant\n",
    "        # --------------------------------------------------------\n",
    "        if code == 1:\n",
    "            temp = subset # Aucune transformation à appliquer\n",
    "        elif code == 2:\n",
    "            temp = subset.diff(periods=1) # Différence première\n",
    "        elif code == 3: \n",
    "            temp = subset.diff(periods=1).diff(periods=1) # Différence seconde\n",
    "        elif code == 4:\n",
    "            temp = np.log(subset) # Transformée en log\n",
    "        elif code == 5:\n",
    "            temp = np.log(subset).diff(periods=1) # Différence première du log\n",
    "        elif code == 6:\n",
    "            temp = np.log(subset).diff(periods=1).diff(periods=1) # Différence seconde du log \n",
    "        elif code == 7:\n",
    "            temp = subset.pct_change().diff(periods=1) # Différence première de la variation relative\n",
    "        # --------------------------------------------------------\n",
    "\n",
    "        # Aggrégation des séries transformées\n",
    "        result = pd.concat([result, temp], axis=1)\n",
    "    # Renvoi des séries transformées et abandonnées des NaN's\n",
    "    return result.dropna(axis=0)\n",
    "\n",
    "def remove_outliers(transformed_data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\" \n",
    "    Traite les séries transformées de la base fred-md de leurs outliers.\n",
    "    Un point x est considéré comme outlier si abs(x - mediane) > 10 * gamme interquartile.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    transformed_data : pd.DataFrame\n",
    "        Tableau des données macro-économiques transformées de la base fred-md\n",
    "    \"\"\"\n",
    "    # Calcul de la médiane des séries\n",
    "    medians = transformed_data.median(axis=0)\n",
    "    # DataFrame contenant les médianes des séries en chaque point\n",
    "    mdf =  transformed_data * 0 + medians\n",
    "    # Calcul de la distance entre les observations et les médianes\n",
    "    z = abs(transformed_data - mdf)\n",
    "    # Calcul de la gamme interquartile des séries\n",
    "    irq = transformed_data.quantile(q=.75) - transformed_data.quantile(q=.25)\n",
    "    # DataFrame contenant les gammes interquartiles des séries en chaque point\n",
    "    irqdf = transformed_data * 0 + irq\n",
    "    # Détermination des outliers selon la régle fournie par McCracken et Ng (2016)\n",
    "    outliers = z > 10 * irqdf\n",
    "    # Abandon des observations considérées comme outliers\n",
    "    mapping = transformed_data[outliers == False].dropna(axis=0)\n",
    "    # Renvoi des séries traitées des outliers\n",
    "    return mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lecture du fichier *.txt\n",
    "data = pd.read_csv('fredmq.txt', sep=',').set_index(keys='Date')\n",
    "# Récupération des types de transformations dans un dictionnaire\n",
    "transformations = dict(data.loc['Transform']) \n",
    "# Abandon de la ligne des transformations\n",
    "data.drop(labels='Transform', inplace=True)\n",
    "# Ajustement de l'indice du DataFrame au format datetime\n",
    "data.index = pd.to_datetime(arg=data.index)\n",
    "# Abandon des colonnes ayant un nombre de NaN's >= 30\n",
    "data.dropna(thresh=len(data) - 30, axis=1, inplace=True)\n",
    "# Abandon des NaN's restants\n",
    "data.dropna(axis=0, inplace=True)\n",
    "# Transformation des séries \n",
    "transformed_data = fred_transform(data, transformations)\n",
    "# Traitement des outliers\n",
    "mapping = remove_outliers(transformed_data)\n",
    "# Sélection de la période d'estimation\n",
    "estimation_sample = mapping.loc[(mapping.index >= pd.to_datetime('1983-01-01')) \n",
    "                                & (mapping.index <= pd.to_datetime('2016-12-01'))]\n",
    "# Sélection de la période de prévision\n",
    "forecast_sample = mapping.loc[mapping.index >= pd.to_datetime('2017-01-01')]\n",
    "# Standardisation des données d'estimation\n",
    "estimation_sample_std = (estimation_sample - estimation_sample.mean(axis=0)) / estimation_sample.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def principal_components(Y: np.ndarray, r: int) -> tuple:\n",
    "    \"\"\" \n",
    "    Extrait les facteurs et les charges d'une série de données standardisée par décomposition en valeur singulière.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    Y : np.ndarray\n",
    "        Matrice (TxN) de données standardisées.\n",
    "    r : int\n",
    "        Nombre de facteurs à estimer.\n",
    "\n",
    "    Renvoie\n",
    "    ---------\n",
    "    tuple(np.ndarray, np.ndarray) :\n",
    "        Matrice (Txr) des facteurs estimés, Matrice (Nxr) des charges factorielles estimées.\n",
    "    \"\"\"\n",
    "    # Décomposition en valeurs singulières de Y'*Y\n",
    "    U, _, _ = np.linalg.svd(Y.T @ Y)\n",
    "    # Récupération des r premiers eigenvecteurs\n",
    "    loadings = U[:, :r]\n",
    "    # Estimation des facteurs par projection \n",
    "    factors = Y @ loadings\n",
    "    # Renvoi des facteurs et des charges \n",
    "    return factors, loadings \n",
    "\n",
    "def kalman_filter(Y: np.ndarray, r: int, f0: np.ndarray, \n",
    "                  p0: np.ndarray, lbda: np.ndarray, phi: np.ndarray, \n",
    "                  sigma_e: np.ndarray, sigma_u: np.ndarray=None) -> tuple:\n",
    "    \"\"\" \n",
    "    Applique le filtre de Kalman pour un modèle DFM dans lequel la variable d'etat évolue selon un VAR(p=1).\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    Y : np.ndarray\n",
    "        Matrice (TxN) de données standardisées.\n",
    "    r : int\n",
    "        Nombre de facteurs à estimer.\n",
    "    f0 : np.ndarray \n",
    "        Vecteur (rx1) des facteurs initiaux.\n",
    "    p0 : np.ndarray\n",
    "        Matrice (rxr) de covariance des facteurs initiaux.\n",
    "    lbda : np.ndarray\n",
    "        Matrice (Nxr) des charges factorielles.\n",
    "    phi : np.ndarray\n",
    "        Matrice (rxr) des paramètres autorégressifs du VAR.\n",
    "    sigma_e : np.ndarray\n",
    "        Matrice (NxN) diagonale des variances des composantes idiosyncratiques des N variables.\n",
    "    sigma_u : np.ndarray\n",
    "        Matrice (rxr) de covariance des erreurs ut (Si passé comme None alors sigma_u = Ir).\n",
    "\n",
    "    Renvoie\n",
    "    ---------\n",
    "    tuple(np.ndarray, np.ndarray, float) :\n",
    "        Matrice (Txr) des facteurs filtrés, Vecteur (Tx1) des matrices (NxN) de covariance des facteurs filtrés, log-vraissemblance.\n",
    "    \"\"\"\n",
    "    # Récupération du nombre d'observations et de variables\n",
    "    T, N = Y.shape\n",
    "    # Initialisation de la log-vraissemblance\n",
    "    loglik = 0\n",
    "    # Initialisation du vecteur contenant les facteurs filtrés\n",
    "    f_filtered = np.zeros(shape=(T, r))\n",
    "    # Initialisation du vecteur contenant les matrices de covariance des facteurs filtrés\n",
    "    p_filtered = np.zeros(shape=(T, r, r))\n",
    "    # Initialisation du filtre \n",
    "    f_prev, p_prev = f0, p0\n",
    "    # Vérification du type de sigma_u\n",
    "    if sigma_u is None:\n",
    "        # Si sigma u est None alors sigma_u = Ir \n",
    "        sigma_u = np.eye(r)\n",
    "    # Application du filtre pour t = 1,...,T\n",
    "    for t in range(T):\n",
    "\n",
    "        # Sélection du vecteur (Nx1) des données observables en t\n",
    "        yt = Y[t, :].reshape((N, 1))\n",
    "    \n",
    "        # ---------------------------------\n",
    "        # Etape n°1 -> prédiction\n",
    "        # ---------------------------------\n",
    "        f_pred = phi @ f_prev # Calcul de ft/t-1\n",
    "        p_pred = phi @ p_prev @ phi.T + sigma_u # Calcul de Pt/t-1\n",
    "        sigma = lbda @ p_pred @ lbda.T + sigma_e # Calcul de sigma_t/t-1 (matrice de var-cov des innovations)\n",
    "        # ---------------------------------\n",
    "        # Etape n°2 -> mise à jour\n",
    "        # ---------------------------------\n",
    "        sigma_e_inv = np.linalg.inv(sigma_e) # Inversion de la matrice sigma_e\n",
    "        sigma_inv = sigma_e_inv - (sigma_e_inv @ lbda) @ np.linalg.inv(np.linalg.pinv(p_pred) + lbda.T @ sigma_e_inv @ lbda) @ (lbda.T @ sigma_e_inv) # Calcul de l'inverse de sigma_t/t-1 via l'identité de Woodburry\n",
    "        nu = yt - lbda @ f_pred # Calcul de l'innovation\n",
    "        kt = p_pred @ lbda.T @ sigma_inv # Calcul de la matrice de gain de Kalman\n",
    "        f_est = f_pred + kt @ nu # Calcul de ft/t\n",
    "        p_est = p_pred - kt @ lbda @ p_pred # Calcul de Pt/t\n",
    "        # ---------------------------------\n",
    "\n",
    "        # Calcul de la log-vraissemblance en t\n",
    "        loglik += -.5 * (np.log(np.linalg.det(sigma)) + nu.T @ sigma_inv @ nu)\n",
    "        # Sauvegarde de l'etat filtré\n",
    "        f_filtered[t] = f_est.flatten() # .flatten() s'assure que f_est soit unidimensionnel\n",
    "        # Sauvegarde de la matrice de covariance de l'etat filtré\n",
    "        p_filtered[t] = p_est\n",
    "        # Mise a jour des estimations précédentes\n",
    "        f_prev, p_prev = f_est, p_est\n",
    "\n",
    "    # Calcul final de la log-vraissemblance\n",
    "    loglik += -.5 * T * N  * np.log(2 * np.pi)\n",
    "    # Renvoi des etats filtrés, des matrices de covariance et de la log-vraissemblance\n",
    "    return f_filtered, p_filtered, loglik.item() # .item() car loglik est sous forme ndarray à cause des opérations vectorielles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsls(Y: np.ndarray, r: int) -> tuple:\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    # Récupération du nombre d'observations\n",
    "    T, _ = Y.shape\n",
    "    # Estimation des facteurs et charges par analyse en composantes principales \n",
    "    pc_factors, pc_loadings = principal_components(Y, r)\n",
    "    # Estimation des paramètres autorégressifs à l'aide des facteurs extraits\n",
    "    phi0 = np.linalg.inv(pc_factors[:-1].T @ pc_factors[:-1]) @ (pc_factors[:-1].T @ pc_factors[1:])\n",
    "    # Calcul des résidus estimés\n",
    "    epsilon_tilde = Y - pc_factors @ pc_loadings.T\n",
    "    # Calcul de la matrice de covariance (diagonale) des composants idiosyncratiques\n",
    "    sigma_e0 = np.diag(np.sum(epsilon_tilde ** 2, axis=0) / T)\n",
    "    # Calcul des erreurs u\n",
    "    u_tilde = pc_factors[1:] - pc_factors[:-1] @ phi0.T\n",
    "    # Calcul de la matrice de covariance des erreurs u\n",
    "    sigma_u0 = (u_tilde.T @ u_tilde) / T\n",
    "    # Initialisation du vecteur des facteurs initiaux par les moyennes inconditionnelles des facteurs extraits par PC\n",
    "    f0 = np.mean(pc_factors, axis=0).reshape((r, 1))\n",
    "    # Initialisation de la matrice de covariance des facteurs initiale par la covariance inconditionnelle des facteurs extraits par PC\n",
    "    p0 = np.cov(pc_factors.T).reshape((r, r))\n",
    "    # Application du filtre de Kalman à l'aide des matrices estimées par PC\n",
    "    f_filtered, p_filtered, _ = kalman_filter(\n",
    "        Y=Y,\n",
    "        r=r,\n",
    "        f0=f0,\n",
    "        p0=p0,\n",
    "        lbda=pc_loadings,\n",
    "        phi=phi0,\n",
    "        sigma_e=sigma_e0,\n",
    "        sigma_u=sigma_u0\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
