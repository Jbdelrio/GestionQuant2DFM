{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as sp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use(style='bmh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fred_transform(data: pd.DataFrame, transformations: dict[str, int]) -> pd.DataFrame:\n",
    "    \"\"\" \n",
    "    Transforme les séries de données macro-économiques selon les recommandations fournies par McCracken et Ng (2016).\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    data : pd.DataFrame\n",
    "        Tableau de données macro-économiques extraties de la base fred-md.\n",
    "     transformations : dict[str, int]\n",
    "        Dictionnaire ayant pour clé le nom de la série et pour valeur son code de transformation associé.\n",
    "\n",
    "    Renvoie\n",
    "    ---------\n",
    "    pd.DataFrame :\n",
    "        Tableau des séries stationnarisées.\n",
    "    \"\"\"\n",
    "    # Création du DataFrame stockant les séries transformées\n",
    "    result = pd.DataFrame()\n",
    "    # Transformation des séries selon les codes fournis par McCracken et Ng (2016)\n",
    "    for col in data.columns:\n",
    "        # Récupération du code de transformation\n",
    "        code = transformations[col]\n",
    "        # Récupération de la série individuelle\n",
    "        subset = data[col]\n",
    "\n",
    "        # Application des transformées selon le code correspondant\n",
    "        # --------------------------------------------------------\n",
    "        if code == 1:\n",
    "            temp = subset # Aucune transformation à appliquer\n",
    "        elif code == 2:\n",
    "            temp = subset.diff(periods=1) # Différence première\n",
    "        elif code == 3: \n",
    "            temp = subset.diff(periods=1).diff(periods=1) # Différence seconde\n",
    "        elif code == 4:\n",
    "            temp = np.log(subset) # Transformée en log\n",
    "        elif code == 5:\n",
    "            temp = np.log(subset).diff(periods=1) # Différence première du log\n",
    "        elif code == 6:\n",
    "            temp = np.log(subset).diff(periods=1).diff(periods=1) # Différence seconde du log \n",
    "        elif code == 7:\n",
    "            temp = subset.pct_change().diff(periods=1) # Différence première de la variation relative\n",
    "        # --------------------------------------------------------\n",
    "\n",
    "        # Aggrégation des séries transformées\n",
    "        result = pd.concat([result, temp], axis=1)\n",
    "    # Renvoi des séries transformées et abandonnées des NaN's\n",
    "    return result.dropna(axis=0)\n",
    "\n",
    "def remove_outliers(transformed_data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\" \n",
    "    Traite les séries transformées de la base fred-md de leurs outliers.\n",
    "    Un point x est considéré comme outlier si abs(x - mediane) > 10 * gamme interquartile.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    transformed_data : pd.DataFrame\n",
    "        Tableau des données macro-économiques transformées de la base fred-md.\n",
    "\n",
    "    Renvoie\n",
    "    ---------\n",
    "    pd.DataFrame :\n",
    "        Tableau des séries transformées corrigées des outliers.\n",
    "    \"\"\"\n",
    "    # Calcul de la médiane des séries\n",
    "    medians = transformed_data.median(axis=0)\n",
    "    # DataFrame contenant les médianes des séries en chaque point\n",
    "    mdf =  transformed_data * 0 + medians\n",
    "    # Calcul de la distance entre les observations et les médianes\n",
    "    z = abs(transformed_data - mdf)\n",
    "    # Calcul de la gamme interquartile des séries\n",
    "    irq = transformed_data.quantile(q=.75) - transformed_data.quantile(q=.25)\n",
    "    # DataFrame contenant les gammes interquartiles des séries en chaque point\n",
    "    irqdf = transformed_data * 0 + irq\n",
    "    # Détermination des outliers selon la régle fournie par McCracken et Ng (2016)\n",
    "    outliers = z > 10 * irqdf\n",
    "    # Abandon des observations considérées comme outliers\n",
    "    mapping = transformed_data[outliers == False].dropna(axis=0)\n",
    "    # Renvoi des séries traitées des outliers\n",
    "    return mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lecture du fichier *.txt\n",
    "data = pd.read_csv('fredmq.txt', sep=',').set_index(keys='Date')\n",
    "# Récupération des types de transformations dans un dictionnaire\n",
    "transformations = dict(data.loc['Transform']) \n",
    "# Abandon de la ligne des transformations\n",
    "data.drop(labels='Transform', inplace=True)\n",
    "# Ajustement de l'indice du DataFrame au format datetime\n",
    "data.index = pd.to_datetime(arg=data.index)\n",
    "# Abandon des colonnes ayant un nombre de NaN's >= 30\n",
    "data.dropna(thresh=len(data) - 30, axis=1, inplace=True)\n",
    "# Abandon des NaN's restants\n",
    "data.dropna(axis=0, inplace=True)\n",
    "# Transformation des séries \n",
    "transformed_data = fred_transform(data, transformations)\n",
    "# Traitement des outliers\n",
    "mapping = remove_outliers(transformed_data)\n",
    "# Sélection de la période d'estimation\n",
    "estimation_sample = mapping.loc[(mapping.index >= pd.to_datetime('1983-01-01')) \n",
    "                                & (mapping.index <= pd.to_datetime('2016-12-01'))]\n",
    "# Sélection de la période de prévision\n",
    "forecast_sample = mapping.loc[mapping.index >= pd.to_datetime('2017-01-01')]\n",
    "# Standardisation des données d'estimation\n",
    "estimation_sample_std = (estimation_sample - estimation_sample.mean(axis=0)) / estimation_sample.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pc(Y: np.ndarray, r: int) -> tuple:\n",
    "    \"\"\" \n",
    "    Extrait les facteurs et les charges d'une série de données standardisée par analyse en composantes principales.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    Y : np.ndarray\n",
    "        Matrice (TxN) de données standardisées.\n",
    "    r : int\n",
    "        Nombre de facteurs à estimer.\n",
    "\n",
    "    Renvoie\n",
    "    ---------\n",
    "    tuple :\n",
    "        Matrice (Txr) des facteurs estimés, \n",
    "        Matrice (Nxr) des charges factorielles estimées,\n",
    "        Vecteur (rx1) des r plus grandes valeurs propres.\n",
    "    \"\"\"\n",
    "    # Calcul de la matrice de covariance de Y\n",
    "    covmat = np.cov(Y, rowvar=False)\n",
    "    # Calcul des valeurs et vecteurs propres\n",
    "    eigvals, eigvecs = np.linalg.eig(covmat)\n",
    "    # Récupération des indices des valeurs propres dans l'ordre décroissant\n",
    "    idx = np.argsort(eigvals)[::-1]\n",
    "    # Tri des valeurs propres dans l'ordre décroissant\n",
    "    sorted_eigvals = eigvals[idx]\n",
    "    # Tri des vecteurs propres selon leur valeur propre associée\n",
    "    sorted_eigvecs = eigvecs[:, idx]\n",
    "    # Récupération des r premiers vecteurs propres\n",
    "    loadings = sorted_eigvecs[:, :r]\n",
    "    # Estimation des facteurs par projection\n",
    "    factors = Y @ loadings\n",
    "    # Renvoi des facteurs, charges factorielles et r premières valeurs propres\n",
    "    return factors, loadings, sorted_eigvals[:r]\n",
    "\n",
    "def kalman_filter(Y: np.ndarray, r: int, f0: np.ndarray, \n",
    "                  p0: np.ndarray, lbda: np.ndarray, phi: np.ndarray, \n",
    "                  sigma_e: np.ndarray, sigma_u: np.ndarray=None) -> tuple:\n",
    "    \"\"\" \n",
    "    Applique le filtre de Kalman pour un modèle DFM dans lequel la variable d'etat évolue selon un VAR(p=1).\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    Y : np.ndarray\n",
    "        Matrice (TxN) de données standardisées.\n",
    "    r : int\n",
    "        Nombre de facteurs à estimer.\n",
    "    f0 : np.ndarray \n",
    "        Vecteur (rx1) des facteurs initiaux.\n",
    "    p0 : np.ndarray\n",
    "        Matrice (rxr) de covariance des facteurs initiaux.\n",
    "    lbda : np.ndarray\n",
    "        Matrice (Nxr) des charges factorielles.\n",
    "    phi : np.ndarray\n",
    "        Matrice (rxr) des paramètres autorégressifs du modèle VAR.\n",
    "    sigma_e : np.ndarray\n",
    "        Matrice (NxN) diagonale des variances des composantes idiosyncratiques des N variables.\n",
    "    sigma_u : np.ndarray\n",
    "        Matrice (rxr) de covariance des erreurs ut (Si passé comme None alors sigma_u = Ir).\n",
    "\n",
    "    Renvoie\n",
    "    ---------\n",
    "    tuple :\n",
    "        Matrice (Txr) des facteurs filtrés, \n",
    "        Vecteur (Tx1) des matrices (NxN) de covariance des facteurs filtrés, \n",
    "        Vecteur (Tx1) des matrices (NxN) de covariance des facteurs prédites, \n",
    "        Log-vraissemblance du modèle DFM.\n",
    "    \"\"\"\n",
    "    # Récupération du nombre d'observations et de variables\n",
    "    T, N = Y.shape\n",
    "    # Initialisation de la log-vraissemblance\n",
    "    loglik = 0\n",
    "    # Initialisation de la matrice contenant les facteurs filtrés\n",
    "    f_filtered = np.zeros(shape=(T, r))\n",
    "    # Initialisation du vecteur contenant les matrices de covariance des facteurs filtrés\n",
    "    p_filtered = np.zeros(shape=(T, r, r))\n",
    "    # Initialisation du vecteur contenant les matrices de covariance des facteurs prédites\n",
    "    p_predicted = np.zeros(shape=(T, r, r))\n",
    "    # Initialisation du filtre \n",
    "    f_prev, p_prev = f0, p0\n",
    "    # Vérification du type de sigma_u\n",
    "    if sigma_u is None:\n",
    "        # Si sigma_u est None alors sigma_u = Ir \n",
    "        sigma_u = np.eye(r)\n",
    "    # Application du filtre pour t = 1,...,T\n",
    "    for t in range(T):\n",
    "\n",
    "        # Sélection du vecteur (Nx1) des données observables en t\n",
    "        yt = Y[t, :].reshape((N, 1))\n",
    "    \n",
    "        # ---------------------------------\n",
    "        # Etape n°1 -> prédiction\n",
    "        # ---------------------------------\n",
    "        f_pred = phi @ f_prev # Calcul de ft/t-1\n",
    "        p_pred = phi @ p_prev @ phi.T + sigma_u # Calcul de Pt/t-1\n",
    "        sigma = lbda @ p_pred @ lbda.T + sigma_e # Calcul de sigma_t/t-1 (matrice de var-cov des innovations)\n",
    "        \n",
    "        # ---------------------------------\n",
    "        # Etape n°2 -> mise à jour\n",
    "        # ---------------------------------\n",
    "        sigma_e_inv = np.linalg.pinv(sigma_e) # Inversion de la matrice sigma_e\n",
    "        sigma_inv = sigma_e_inv - (sigma_e_inv @ lbda) @ np.linalg.pinv(np.linalg.pinv(p_pred) + lbda.T @ sigma_e_inv @ lbda) @ (lbda.T @ sigma_e_inv) # Calcul de l'inverse de sigma_t/t-1 via l'identité de Woodburry\n",
    "        nu = yt - lbda @ f_pred # Calcul de l'innovation\n",
    "        kt = p_pred @ lbda.T @ sigma_inv # Calcul de la matrice de gain de Kalman\n",
    "        f_est = f_pred + kt @ nu # Calcul de ft/t\n",
    "        p_est = p_pred - kt @ lbda @ p_pred # Calcul de Pt/t\n",
    "        # ---------------------------------\n",
    "\n",
    "        # Calcul de la log-vraissemblance en t\n",
    "        loglik += -.5 * (np.log(np.linalg.det(sigma)) + nu.T @ sigma_inv @ nu)\n",
    "        # Sauvegarde de l'etat filtré\n",
    "        f_filtered[t] = f_est.flatten() # .flatten() s'assure que f_est soit unidimensionnel\n",
    "        # Sauvegarde de la matrice de covariance de l'etat filtré\n",
    "        p_filtered[t] = p_est\n",
    "        # Sauvegarde de la matrice de covariance prédite\n",
    "        p_predicted[t] = p_pred\n",
    "        # Mise a jour des estimations précédentes\n",
    "        f_prev, p_prev = f_est, p_est\n",
    "\n",
    "    # Calcul final de la log-vraissemblance\n",
    "    loglik += -.5 * T * N  * np.log(2 * np.pi)\n",
    "    # Renvoi des etats filtrés, des matrices de covariance et de la log-vraissemblance\n",
    "    return f_filtered, p_filtered, p_predicted, loglik.item() # .item() car loglik est sous forme ndarray à cause des opérations vectorielles\n",
    "\n",
    "def kalman_smoother(phi: np.ndarray, f_filtered: np.ndarray, \n",
    "                    p_filtered: np.ndarray, p_predicted: np.ndarray) -> tuple:\n",
    "    \"\"\" \n",
    "    Applique le lissage de Kalman pour un modèle DFM dans lequel la variable d'etat évolue selon un VAR(p=1).\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    phi : np.ndarray\n",
    "        Matrice (rxr) des paramètres autorégressifs du modèle VAR.\n",
    "    f_filtered : np.ndarray\n",
    "        Matrice (Txr) des facteurs filtrés estimés via le filtre de Kalman.\n",
    "    p_filtered : np.ndarray\n",
    "        Vecteur (Tx1) des matrices (NxN) de covariance des facteurs filtrés estimés via le filtre de Kalman.\n",
    "    p_predicted : np.ndarray\n",
    "        Vecteur (Tx1) des matrices (NxN) de covariance des facteurs prédites via le filtre de Kalman.\n",
    "    \n",
    "    Renvoie\n",
    "    ---------\n",
    "    tuple :\n",
    "        Vecteur (Txr) des facteurs lissés, \n",
    "        Vecteur (Tx1) des matrices (NxN) de covariance des facteurs lissés, \n",
    "        Vecteur (T-1x1) des matrices (rxr) de gain de lissage.    \n",
    "    \"\"\"\n",
    "    # Récupération du nombre d'observations et de facteurs\n",
    "    T, r = f_filtered.shape\n",
    "    # Initialisation de la matrice contenant les états lissés\n",
    "    f_smoothed = np.zeros(shape=(T, r))\n",
    "    # Initialisation du vecteur contenant les matrices de covariance des facteurs lissés\n",
    "    p_smoothed = np.zeros(shape=(T, r, r))\n",
    "    # Initialisation du vecteur contenant les matrices de gain de lissage (Taille T par soucis d'indexation)\n",
    "    C = np.zeros(shape=(T, r, r)) \n",
    "    # Initialisation des facteurs lissées : état lissé en T = état filtré en T\n",
    "    f_smoothed[-1] = f_filtered[-1]\n",
    "    # Initialisation des matrices de covariance lissées\n",
    "    p_smoothed[-1] = p_filtered[-1]\n",
    "    # Application du lissage de Kalman pour t = T-1,...,1\n",
    "    for t in range(T-2, -1, -1):\n",
    "        # Récupération de ft/t et ft+1/T\n",
    "        f_est, f_fwd = f_filtered[t], f_smoothed[t+1]\n",
    "        # Récupération de Pt/t, Pt+1/t et Pt+1/T\n",
    "        p_est, p_pred, p_fwd = p_filtered[t], p_predicted[t+1], p_smoothed[t+1]\n",
    "        # Calcul du gain de lissage\n",
    "        ct = p_est @ phi.T @ np.linalg.pinv(p_pred)\n",
    "        # Calcul et sauvegarde du facteur lissé ft/T\n",
    "        f_smoothed[t] = f_est + ct @ (f_fwd - phi @ f_est)\n",
    "        # Calcul et sauvegarde de la matrice de covariance lissée\n",
    "        p_smoothed[t] = p_est + ct @ (p_fwd - p_pred) @ ct.T\n",
    "        # Sauvegarde de la matrice de gain de lissage Ct\n",
    "        C[t+1] = ct \n",
    "    # Renvoi des etats lissés, matrices de covariance lissées et matrices de gain de lissage\n",
    "    return f_smoothed, p_smoothed, C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsls(Y: np.ndarray, r: int) -> tuple:\n",
    "    \"\"\"\n",
    "    Estime les r facteurs latents d'un modèle DFM à l'aide de la méthode Two Step Least Squares (Doz et al. - 2011).\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    Y : np.ndarray\n",
    "        Matrice (TxN) de données standardisées.\n",
    "    r : int\n",
    "        Nombre de facteurs à estimer.\n",
    "\n",
    "    Renvoie\n",
    "    ---------\n",
    "    tuple :\n",
    "        Matrice (Txr) des facteurs lissés par l'algorithme KFS,\n",
    "        Matrice (TxN) des charges factorielles extraites par PC,\n",
    "        Matrice (rxr) des paramètres autorégressifs estimés à partir des facteurs extraits par PC, \n",
    "        Vecteur (Nx1) des variances des composantes idiosyncratiques.\n",
    "    \"\"\"\n",
    "    # Récupération du nombre d'observations\n",
    "    T, _ = Y.shape\n",
    "    # Estimation des facteurs et charges par analyse en composantes principales \n",
    "    pc_factors, pc_loadings, _ = pc(Y, r)\n",
    "    # Estimation des paramètres autorégressifs à l'aide des facteurs extraits\n",
    "    phi0 = np.linalg.pinv(pc_factors[:-1].T @ pc_factors[:-1]) @ (pc_factors[:-1].T @ pc_factors[1:])\n",
    "    # Calcul des résidus estimés\n",
    "    epsilon_tilde = Y - pc_factors @ pc_loadings.T\n",
    "    # Calcul de la matrice de covariance (diagonale) des composants idiosyncratiques\n",
    "    sigma_e0 = np.diag(np.sum(epsilon_tilde ** 2, axis=0) / T)\n",
    "    # Calcul des erreurs u\n",
    "    u_tilde = pc_factors[1:] - pc_factors[:-1] @ phi0.T\n",
    "    # Calcul de la matrice de covariance des erreurs u\n",
    "    sigma_u0 = (u_tilde.T @ u_tilde) / T\n",
    "    # Initialisation du vecteur des facteurs initiaux par les moyennes inconditionnelles des facteurs extraits par PC\n",
    "    f0 = np.mean(pc_factors, axis=0).reshape((r, 1))\n",
    "    # Initialisation de la matrice de covariance des facteurs initiale par la covariance inconditionnelle des facteurs extraits par PC\n",
    "    p0 = np.cov(pc_factors.T).reshape((r, r))\n",
    "    # Application du filtre de Kalman à l'aide des matrices estimées par PC\n",
    "    f_filtered, p_filtered, p_predicted, _ = kalman_filter(\n",
    "        Y=Y,\n",
    "        r=r,\n",
    "        f0=f0,\n",
    "        p0=p0,\n",
    "        lbda=pc_loadings,\n",
    "        phi=phi0,\n",
    "        sigma_e=sigma_e0,\n",
    "        sigma_u=sigma_u0\n",
    "    )\n",
    "    # Application du lissage de Kalman à partir des états filtrés\n",
    "    f_smoothed, _, _ = kalman_smoother(phi=phi0, f_filtered=f_filtered, \n",
    "                                    p_filtered=p_filtered, p_predicted=\tp_predicted)\n",
    "    # Renvoi des facteurs lissés, charges factorielles, paramètres autorégressifs et variances idiosyncratiques\n",
    "    return f_smoothed, pc_loadings, phi0, np.diag(sigma_e0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_lbda(Y: np.ndarray, f_smoothed: np.ndarray, p_smoothed: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calcule la matrice des charges factorielles maximisant l'espérance conditionnelle \n",
    "    de la log-vraissemblance du modèle DFM à partir des états et covariances lissées par KFS.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    Y : np.ndarray\n",
    "        Matrice (TxN) de données standardisées.\n",
    "    f_smoothed : np.ndarray\n",
    "        Vecteur (Txr) des facteurs lissés.\n",
    "    p_smoothed : np.ndarray\n",
    "        Vecteur (Tx1) des matrices (NxN) de covariance des facteurs lissés.\n",
    "\n",
    "    Renvoie\n",
    "    ---------\n",
    "    np.ndarray :\n",
    "        Matrice (Nxr) des charges factorielles maximisant l'espérance conditionnelle de la log-vraissemblance.\n",
    "    \"\"\"\n",
    "    # Nombre d'observations et de facteurs\n",
    "    T, r = f_smoothed.shape\n",
    "    # Pré-allocation de mémoire pour le dénominateur\n",
    "    den = np.zeros(shape=(r, r))\n",
    "    # Calcul du numérateur -> Yt * ft/T\n",
    "    num = Y.T @ f_smoothed\n",
    "    # Calcul itératif du dénominateur pour t = 1,...,T\n",
    "    for t in range(T):\n",
    "        # ft/T * ft/T' + Pt/T\n",
    "        den += f_smoothed[t, :].reshape((r, 1)) @ f_smoothed[t, :].reshape((1, r)) + p_smoothed[t, :, :]\n",
    "    # Renvoi des charges factorielles mises à jour\n",
    "    return num @ np.linalg.pinv(den)\n",
    "\n",
    "def update_phi(f_smoothed: np.ndarray, p_smoothed: np.ndarray, C: np.ndarray) -> np.ndarray:\n",
    "    \"\"\" \n",
    "    Calcule la matrice des paramètres autorégressifs maximisant l'espérance conditionnelle \n",
    "    de la log-vraissemblance du modèle DFM à partir des états, covariances lissées par KFS \n",
    "    et matrices de gain de lissage.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    f_smoothed : np.ndarray\n",
    "        Vecteur (Txr) des facteurs lissés.\n",
    "    p_smoothed : np.ndarray\n",
    "        Vecteur (Tx1) des matrices (NxN) de covariance des facteurs lissés.\n",
    "    C : np.ndarray\n",
    "        Vecteur (T-1x1) des matrices (rxr) de gain de lissage.    \n",
    "\n",
    "    Renvoie\n",
    "    ---------\n",
    "    np.ndarray :\n",
    "        Matrice (rxr) des paramètres autorégressifs maximisant l'espérance conditionnelle de la log-vraissemblance.\n",
    "    \"\"\"\n",
    "    # Nombre d'observations et de facteurs\n",
    "    T, r = f_smoothed.shape\n",
    "    # Préallocation de mémoire pour le numérateur et dénominateur\n",
    "    num, den = np.zeros(shape=(r, r)), np.zeros(shape=(r, r))\n",
    "    # Calcul itératif du numérateur pour t = 2, ..., T\n",
    "    for t in range(1, T):\n",
    "        # ft/T * ft-1/T + Ct\n",
    "        num += f_smoothed[t, :].reshape((r, 1)) @ f_smoothed[t-1, :].reshape((1, r)) + C[t, :, :]\n",
    "    # Calcul itératif du dénominateur pour t = 1,...,T-1\n",
    "    for t in range(T-1):\n",
    "        # ft-1/T * ft-1/T' + Pt/T\n",
    "        den += f_smoothed[t, :].reshape((r, 1)) @ f_smoothed[t, :].reshape((1, r)) + p_smoothed[t, :, :]\n",
    "    # Renvoi des paramètres autorégressif mis à jour\n",
    "    return num @ np.linalg.pinv(den)\n",
    "\n",
    "def e_step(Y: np.ndarray, r: int, f0: np.ndarray,\n",
    "           p0: np.ndarray, lbda: np.ndarray, \n",
    "           phi: np.ndarray, sigma_e: np.ndarray) -> tuple:\n",
    "    \"\"\"\n",
    "    Etape E de l'algorithme Espérance-Maximisation.\n",
    "\n",
    "    Applique le Kalman Filter and Smoother (KFS) à l'aide des paramètres fournis.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    Y : np.ndarray\n",
    "        Matrice (TxN) de données standardisées.\n",
    "    r : int\n",
    "        Nombre de facteurs à estimer.\n",
    "    f0 : np.ndarray \n",
    "        Vecteur (rx1) des facteurs initiaux.\n",
    "    p0 : np.ndarray\n",
    "        Matrice (rxr) de covariance des facteurs initiaux.\n",
    "    lbda : np.ndarray\n",
    "        Matrice (Nxr) des charges factorielles.\n",
    "    phi : np.ndarray\n",
    "        Matrice (rxr) des paramètres autorégressifs du modèle VAR.\n",
    "    sigma_e : np.ndarray\n",
    "        Matrice (NxN) diagonale des variances des composantes idiosyncratiques des N variables.\n",
    "\n",
    "    Renvoie\n",
    "    ---------\n",
    "    tuple :\n",
    "        Vecteur (Txr) des facteurs lissés, \n",
    "        Vecteur (Tx1) des matrices (NxN) de covariance des facteurs lissés, \n",
    "        Vecteur (T-1x1) des matrices (rxr) de gain de lissage,\n",
    "        Log-vraissemblance du modèle DFM.\n",
    "    \"\"\"\n",
    "    # Application du filtre de Kalman\n",
    "    f_filtered, p_filtered, p_predicted, loglik = kalman_filter(\n",
    "        Y=Y,\n",
    "        r=r,\n",
    "        f0=f0,\n",
    "        p0=p0,\n",
    "        lbda=lbda,\n",
    "        phi=phi,\n",
    "        sigma_e=sigma_e,\n",
    "        sigma_u=None # A des fins d'identification, nous imposons sigma_u = Ir\n",
    "    )\n",
    "    # Application du lissage de Kalman\n",
    "    f_smoothed, p_smoothed, C = kalman_smoother(phi=phi, f_filtered=f_filtered,\n",
    "                                                p_filtered=p_filtered, p_predicted=p_predicted)\n",
    "    # Renvoi des facteurs lissés, matrices de covariance lissées, matrices de gain de lissage, log-vraissemblance\n",
    "    return f_smoothed, p_smoothed, C, loglik\n",
    "    \n",
    "def m_step(Y: np.ndarray, f_smoothed: np.ndarray,\n",
    "           p_smoothed: np.ndarray, C: np.ndarray,\n",
    "           lbda_prev: np.ndarray) -> tuple:\n",
    "    \"\"\"\n",
    "    Etape M de l'algorithme Espérance-Maximisation.\n",
    "\n",
    "    Détermine les paramètres maximisant l'espérance conditionnelle de la log-vraissemblance du modèle DFM. \n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    Y : np.ndarray\n",
    "        Matrice (TxN) de données standardisées.\n",
    "    f_smoothed : np.ndarray\n",
    "        Vecteur (Txr) des facteurs lissés.\n",
    "    p_smoothed : np.ndarray\n",
    "        Vecteur (Tx1) des matrices (NxN) de covariance des facteurs lissés.\n",
    "    C : np.ndarray\n",
    "        Vecteur (T-1x1) des matrices (rxr) de gain de lissage.    \n",
    "    lbda : np.ndarray\n",
    "        Matrice (Nxr) des charges factorielles.\n",
    "\n",
    "    Renvoie\n",
    "    ---------\n",
    "    tuple :\n",
    "        Matrice (Nxr) des charges factorielles mises à jour,\n",
    "        Matrice (rxr) des paramètres autorégressifs du modèle VAR mis à jour.\n",
    "        Matrice (NxN) diagonale des variances des composantes idiosyncratiques des N variables mises à jour.\n",
    "    \"\"\"\n",
    "    # Nombre d'observations\n",
    "    T, _ = Y.shape\n",
    "    # Calcul des résidus estimés\n",
    "    epsilon_tilde = Y - f_smoothed @ lbda_prev.T\n",
    "    # Calcul de la matrice de covariance (diagonale) des composants idiosyncratiques mise à jour\n",
    "    sigma_e = np.diag(np.sum(epsilon_tilde ** 2, axis=0) / T)\n",
    "    # Calcul des charges factorielles mises à jour\n",
    "    lbda = update_lbda(Y, f_smoothed, p_smoothed)\n",
    "    # Calcul des paramètres autorégressifs mis à jour \n",
    "    phi = update_phi(f_smoothed, p_smoothed, C)\n",
    "    # Renvoi des charges factorielles, paramètres autorégressifs et variances idiosyncratiques mises à jour\n",
    "    return lbda, phi, sigma_e\n",
    "\n",
    "def mlem(Y: np.ndarray, r: int, tol: float=1e-4, max_iter: int=100) -> tuple:\n",
    "    \"\"\"\n",
    "    Algotithme Espérance-Maximisation\n",
    "    \"\"\"\n",
    "    # Récupération du nombre d'observations\n",
    "    T, _ = Y.shape\n",
    "    # Estimation des facteurs et charges par analyse en composantes principales \n",
    "    pc_factors, pc_loadings, _ = pc(Y, r)\n",
    "    # Estimation des paramètres autorégressifs à l'aide des facteurs extraits\n",
    "    phi0 = np.linalg.pinv(pc_factors[:-1].T @ pc_factors[:-1]) @ (pc_factors[:-1].T @ pc_factors[1:])\n",
    "    # Calcul des résidus estimés\n",
    "    epsilon_tilde = Y - pc_factors @ pc_loadings.T\n",
    "    # Calcul de la matrice de covariance (diagonale) des composants idiosyncratiques\n",
    "    sigma_e0 = np.diag(np.sum(epsilon_tilde ** 2, axis=0) / T)\n",
    "    # Initialisation du vecteur des facteurs initiaux par les moyennes inconditionnelles des facteurs extraits par PC\n",
    "    f0 = np.mean(pc_factors, axis=0).reshape((r, 1))\n",
    "    # Initialisation de la matrice de covariance des facteurs initiale par la covariance inconditionnelle des facteurs extraits par PC\n",
    "    p0 = np.cov(pc_factors.T).reshape((r, r))\n",
    "\n",
    "    # ---------------------------------\n",
    "    # Initialisation de l'algorithme\n",
    "    # ---------------------------------\n",
    "    lbda = pc_loadings # Matrice des charges factorielles initiale\n",
    "    phi = phi0 # Matrice des paramètres autorégressifs initiale\n",
    "    sigma_e = sigma_e0 # Matrice de covariance diagonale des composants idiosyncratiques initiale\n",
    "    loglik_prev = -1e9 # Valeur initiale de log-vraissemblance arbitrairement petite \n",
    "    # ---------------------------------\n",
    "\n",
    "    for iter in range(max_iter):\n",
    "\n",
    "        # ---------------------------------\n",
    "        # E-Step -> Application du KFS \n",
    "        # ---------------------------------\n",
    "        f_smoothed, p_smoothed, C, loglik_new = e_step(\n",
    "            Y=Y,\n",
    "            r=r,\n",
    "            f0=f0,\n",
    "            p0=p0,\n",
    "            lbda=lbda,\n",
    "            phi=phi,\n",
    "            sigma_e=sigma_e\n",
    "        )\n",
    "        # Différence absolue des log-vraissemblances\n",
    "        err = np.abs(loglik_new - loglik_prev)\n",
    "        # Vérification de la convergence de la log-vraissemblance\n",
    "        if err < tol:\n",
    "            # Message info utilisateur -> convergence atteinte\n",
    "            print(f\"Convergence de l'algorithme EM atteinte lors de l'itération n°{iter + 1} \" \\\n",
    "                  f\"- Différence des log-vraissemblances = {err:2e}\")\n",
    "            # Sortie de boucle si point de convergence atteint\n",
    "            break\n",
    "        # Message info utilisateur -> poursuite de l'algorithme\n",
    "        print(f\"Itération n°{iter + 1} - Différence des log-vraissemblances = {err:2e}\")\n",
    "        # ---------------------------------\n",
    "\n",
    "        # ---------------------------------\n",
    "        # M-Step -> Mise à jour des paramètres\n",
    "        # ---------------------------------\n",
    "        lbda, phi, sigma_e = m_step(\n",
    "            Y=Y,\n",
    "            f_smoothed=f_smoothed,\n",
    "            p_smoothed=p_smoothed,\n",
    "            C=C,\n",
    "            lbda_prev=lbda\n",
    "        )\n",
    "        # ---------------------------------\n",
    "        # Mise à jour de la log-vraissemblance précédemment calculée\n",
    "        loglik_prev = loglik_new\n",
    "    # Renvoi des facteurs lissés, charges factorielles, paramètres autorégressifs, variances idiosyncratiques, log-vraissemblance\n",
    "    return f_smoothed, lbda, phi, np.diag(sigma_e), loglik_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passage des données standardisées au format np.ndarray\n",
    "Y = estimation_sample_std.to_numpy()\n",
    "# Calcul des r = 20 premières valeurs propres de la matrice de covariance de Y par PC\n",
    "_, _, eigvals = pc(Y, r=20)\n",
    "\n",
    "# Scree plot des valeurs propres de la matrice de covariance de Y\n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.plot(np.arange(1, 21, 1), eigvals / 100, marker='o', color='black')\n",
    "plt.xlabel('Nombre de facteurs'), plt.ylabel('Valeurs propres')\n",
    "plt.xticks(ticks=[1, 5, 10, 15, 20])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nombre de facteurs \n",
    "r = 3\n",
    "# Estimation des facteurs latents par PC\n",
    "pc_factors, _, _ = pc(Y, r)\n",
    "# Estimation des facteurs latents par TS-LS\n",
    "tsls_factors, tsls_loadings, tsls_phi, tsls_diag_sigma_e = tsls(Y, r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlem_factors, mlem_loadings, mlem_phi, mlem_diag_sigma_e, mlem_loglik = mlem(Y, r=3, tol=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lag_matrix(X: np.ndarray, lags: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Créé une matrice des valeurs retardées de X.\n",
    "    (Equivalent à la fonction lagmatrix sur MatLab).\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    X : np.ndarray\n",
    "        Matrice (Tx1) de données.\n",
    "    lags : int\n",
    "        Nombre de retards maximum à prendre en compte. \n",
    "\n",
    "    Renvoie\n",
    "    ---------\n",
    "    np.ndarray : \n",
    "        Matrice (Txlags) des valeurs retardées de X. \n",
    "    \"\"\"\n",
    "    # Nombre d'observations\n",
    "    T = len(X)\n",
    "    # Matrice des valeurs retardées de X\n",
    "    X_lag = np.full(shape=(T, lags), fill_value=np.nan)\n",
    "    # Boucle sur le nombre de lags k = 1:lags\n",
    "    for k in range(lags):\n",
    "        # Ajout de la série retardée de k périodes\n",
    "        X_lag[k+1:, k] = X[:-k-1]\n",
    "    # Renvoi de la matrice retardée\n",
    "    return X_lag\n",
    "\n",
    "def augmented_ols(Yi: np.ndarray, q: int, F: np.ndarray=None, s: int=0) -> tuple:\n",
    "    \"\"\"\n",
    "    Estime les paramètres d'une régression linéaire augmentée des facteurs latents dans le cadre d'un modèle DFM\n",
    "    par méthode des moindres carrés ordinaires.\n",
    "\n",
    "    Le modèle inclus une constante, les valeurs retardées de q périodes de Yi ainsi que les r facteurs latents\n",
    "    retardés de s périodes.\n",
    "\n",
    "    Si F et s ne sont pas passés en arguments le modèle estimé est un AR(q).\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    Yi : np.ndarray\n",
    "        Vecteur (Tx1) de données stationnaires.\n",
    "    q : int\n",
    "        Nombre de périodes de retard à considérer pour Yi.\n",
    "    F : np.ndarray\n",
    "        Matrice (Txr) des facteurs latents.\n",
    "    s : int\n",
    "        Nombre de périodes de retard à considérer pour F.\n",
    "\n",
    "    Renvoie\n",
    "    ---------\n",
    "    tuple :\n",
    "        Vecteur (q + s * r + 1x1) des coefficients de régression estimés, \n",
    "        Vecteur (q + s * r + 1x1) des p-valeurs associées,\n",
    "        Ecart-type des résidus de régression, coéfficient de détermination (R2) ajusté.\n",
    "    \"\"\"\n",
    "    # Nombre d'observations\n",
    "    T = len(Yi)\n",
    "    # Détermination du lag maximal\n",
    "    max_lag = max(q, s)\n",
    "    # Sélection des valeurs expliquées de Yi\n",
    "    yy = Yi[max_lag:].reshape((T - max_lag, 1))\n",
    "    # Constante de régression\n",
    "    const = np.ones(shape=(T, 1))\n",
    "    # Récupération des valeurs retardées de Yi\n",
    "    y_lagged = lag_matrix(Yi, lags=q)\n",
    "    # Aggrégation des régrésseurs\n",
    "    xx = np.concatenate([const, y_lagged], axis=1)\n",
    "    # Ajout des facteurs latents si passés en argument de la fonction\n",
    "    if s > 0 and F is not None:\n",
    "        # Détermination du nombre de facteurs\n",
    "        _, r = F.shape\n",
    "        # Récupération des valeurs retardées des r facteurs\n",
    "        ff = np.concatenate([lag_matrix(F[:, i], s) for i in range(r)], axis=1)\n",
    "        # Aggrégation des facteurs retardés aux séries retardées de Yi\n",
    "        xx = np.concatenate([xx, ff], axis=1)\n",
    "    # Abandon des valeurs manquantes\n",
    "    xx = xx[max_lag:, :]\n",
    "    # Nombre de régrésseurs\n",
    "    _, k = xx.shape\n",
    "    # Estimation des coefficients de régression par méthode MCO\n",
    "    coefs = np.linalg.inv(xx.T @ xx) @ xx.T @ yy\n",
    "    # Estimation des valeurs de Yi\n",
    "    yy_hat = xx @ coefs\n",
    "    # Calcul des résidus de régression\n",
    "    eps_hat = yy - yy_hat\n",
    "    # Calcul de la variance estimée des résidus\n",
    "    sigma2_eps = eps_hat.T @ eps_hat / (T - max_lag - k)\n",
    "    # Calcul de la matrice de variance-covariance des résidus estimés \n",
    "    vcv = sigma2_eps * np.linalg.inv(xx.T @ xx)\n",
    "    # Récupération des écarts-types des coefficients estimés \n",
    "    coefs_std = np.sqrt(np.diag(vcv))\n",
    "    # Calcul des t-stats\n",
    "    tstats = coefs.flatten() / coefs_std\n",
    "    # Calcul des p-valeurs\n",
    "    pvals = 2 * (1 - sp.norm.cdf(np.abs(tstats)))\n",
    "    # Calcul du SST\n",
    "    SST = np.sum((yy - np.mean(yy)) ** 2)\n",
    "    # Calcul du SSR\n",
    "    SSR = np.sum(eps_hat ** 2)\n",
    "    # Calcul du coefficient de détermination\n",
    "    R2 = 1 - (SSR / SST)\n",
    "    # Calcul du coefficient de détermination ajusté\n",
    "    R2_adj = 1 - ((1 - R2) * (T - max_lag - 1) / (T - max_lag - k))\n",
    "    # Renvoi des coefs, p-valeurs, écart-type des résidus, R2 ajusté\n",
    "    return coefs, pvals, np.sqrt(sigma2_eps.item()), R2_adj"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
